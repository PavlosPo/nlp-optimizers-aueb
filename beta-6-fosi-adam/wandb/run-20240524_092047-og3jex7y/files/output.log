
Returned ESE function. Lanczos order (m) is 12 .
  0%|                                                               | 0/1209 [00:00<?, ?it/s][38mic| outputs_argmax: tensor([1, 1, 1, 0])
[38mic| outputs_argmax: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[38m                            1, 1, 1])
Found Better model,
Saving model checkpoint at ./model_checkpoint.
Total Validation loss: 0.6593702534013544
[38mic| metrics: {'ACCURACY': 0.6749482401656315,
[38m              'F1_Macro': 0.8049689440993789,
[38m              'LOSS': 0.6593702534013544,
[38m              'MAE': 0.3250517598343685,
[38m              'MCC': 0.057215491464164506,
[38m              'PRECISION': 0.675,
[38m              'RECALL': 0.9969230769230769}






Epoch: 1, Loss: 0.7235:   8%|â–ˆâ–ˆâ–                           | 99/1209 [00:36<06:53,  2.68it/s]
Traceback (most recent call last):
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/main.py", line 102, in <module>
    main()
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/main.py", line 99, in main
    trainer.train_val_test()
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/trainer.py", line 78, in train_val_test
    self.params, self.opt_state, loss, logits = self.step(self.params, self.buffers, batch, self.opt_state)
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/trainer.py", line 284, in step
    updates, opt_state = self.optimizer.update(grads, opt_state, params)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/fosi/torch_optim/fosi_optimizer.py", line 87, in update_fn
    state = _approx_learning_rates_and_eigenvectors(params, state)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/fosi/torch_optim/fosi_optimizer.py", line 46, in _approx_learning_rates_and_eigenvectors
    k_eigenvals, k_eigenvecs = ese_fn(params)
                               ^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/fosi/torch_optim/extreme_spectrum_estimation.py", line 25, in <lambda>
    ese_fn = lambda params: _ese(lanczos_alg_gitted, batch, params, device)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/fosi/torch_optim/extreme_spectrum_estimation.py", line 7, in _ese
    k_largest_eigenvals, k_largest_eigenvecs, l_smallest_eigenvals, l_smallest_eigenvecs = lanczos_alg_gitted(params, batch)
                                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ppoulos/nlp-optimizers/nlp-optimizers-aueb/beta-6-fosi/fosi/torch_optim/lanczos_algorithm.py", line 165, in lanczos_alg_jitted
    k_largest_eigenvecs = (eigvecs_triag.T[order-k_largest:] @ vecs).type(precision)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU